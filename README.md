# Awesome Large Language Diffusion Models üöÄüß†

A curated list of recent and important papers, tools, and resources on **Diffusion Large Language Models (DLMs)**. 

> Contributions are welcome! Please submit a PR or open an issue.

---

## üî• Papers 

- **[LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs](https://arxiv.org/abs/2506.14429)**  
  *TLDR:* First systematic study of long-context capabilities in diffusion LLMs. Proposes LongLLaDA, a training-free extrapolation method, and finds they outperform AR models on some long-context tasks.

- **[MMaDA: Multimodal Large Diffusion Language Models](https://arxiv.org/abs/2505.15809)**  
  *TLDR:* Introduces MMaDA, a unified multimodal diffusion model excelling in textual reasoning, visual understanding, and text-to-image generation. Outperforms several SOTA models across domains.

- **[Large Language Diffusion Models](https://arxiv.org/abs/2502.09992)**  
  *TLDR:* LLaDA is a Transformer-based diffusion LLM trained from scratch. Matches or beats strong AR baselines like LLaMA3 8B, especially in instruction following and reversal tasks.

- **[Dream](https://github.com/HKUNLP/Dream)**  
  *TLDR:* A 7B diffusion LLM that rivals leading AR models in performance, demonstrating the feasibility of scalable diffusion-based LLMs.

- **[Simple and Effective Masked Diffusion Language Models](https://arxiv.org/abs/2406.07524)**  
  *TLDR:* Shows masked diffusion models can nearly match AR performance with simple training tricks and a Rao-Blackwellized objective. Supports semi-autoregressive generation.

- **[Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models](https://arxiv.org/abs/2503.09573)**  
  *TLDR:* Proposes block diffusion models that combine the strengths of AR and diffusion methods. Enables flexible-length generation and improved efficiency using KV caching.

- **[Scaling Diffusion Language Models via Adaptation from Autoregressive Models](https://arxiv.org/abs/2410.17891)**  
  *TLDR:* Converts AR models like GPT-2 and LLaMA into diffusion models (DiffuGPT, DiffuLLaMA) using continual pretraining. Matches AR models on several benchmarks with less data.

- **[Unifying Autoregressive and Diffusion-Based Sequence Generation](https://arxiv.org/abs/2504.06416)**  
  *TLDR:* Proposes hyperschedules and hybrid noise processes to blend AR and diffusion models. Achieves strong perplexity and diverse generation while enabling error correction during decoding.

- **[Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)**  
  *TLDR:* Introduces DDIMs, which speed up diffusion sampling by 10‚Äì50√ó using non-Markovian processes while maintaining high sample quality.

---

## ü§ñ Repos
- Coming soon

---

## üìö Blogs
- Coming soon

---

## ü§ù Contributing

We welcome contributions!

- üìÑ Add new papers  
- üß™ Share codebases or demos  
- üõ† Submit tutorials, blog posts, or benchmarks  

Feel free to open a pull request or issue!

---

Maintained with ‚ù§Ô∏è by [Alessio Devoto](https://alessiodevoto.github.io/)
